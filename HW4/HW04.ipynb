{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d714ab3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# HW4 Speakers Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394023fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949426dc",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa792b",
   "metadata": {},
   "source": [
    "## Download Data (These links are no longer working, I finally downloaded these on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49202bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L \"https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partaa\" -o Dataset.tar.gz.partaa\n",
    "\n",
    "# !curl -L \"https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partab\" -o Dataset.tar.gz.partab\n",
    "\n",
    "# !curl -L \"https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partac\" -o Dataset.tar.gz.partac\n",
    "\n",
    "# !curl -L \"https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partad\" -o Dataset.tar.gz.partad\n",
    "\n",
    "# !copy /b Dataset.tar.gz.part* Dataset.tar.gz\n",
    "\n",
    "# !tar -zxvf Dataset.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeed095",
   "metadata": {},
   "source": [
    "## Manually Unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3e8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22d198",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc49604",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e056f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset, random_split\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b447c5",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6440585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, segment_len=256):\n",
    "        self.data_dir = data_dir\n",
    "        self.segment_len = segment_len\n",
    "\n",
    "        mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "        mapping = json.load(mapping_path.open())\n",
    "        self.speaker2id = mapping[\"speaker2id\"]\n",
    "\n",
    "        metadata_path = Path(data_dir) / \"metadata.json\"\n",
    "        metadata = json.load(open(metadata_path))[\"speakers\"]\n",
    "\n",
    "        self.speaker_num = len(metadata.keys())\n",
    "        self.data = []\n",
    "        for speaker in metadata.keys():\n",
    "            for utterances in metadata[speaker]:\n",
    "                self.data.append(\n",
    "                    [utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feat_path, speaker_id = self.data[index]\n",
    "        mel = torch.load(os.path.join(self.data_dir, feat_path), weights_only=True)\n",
    "\n",
    "        if len(mel) > self.segment_len:\n",
    "            start = random.randint(0, len(mel) - self.segment_len)\n",
    "            mel = torch.FloatTensor(mel[start:start + self.segment_len])\n",
    "        else:\n",
    "            mel = torch.FloatTensor(mel)\n",
    "\n",
    "        speaker_id = torch.tensor(speaker_id, dtype=torch.long)\n",
    "        return mel, speaker_id\n",
    "\n",
    "    def get_speaker_number(self):\n",
    "        return self.speaker_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7040dc5",
   "metadata": {},
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b4dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    mel, speaker = zip(*batch)\n",
    "    mel = pad_sequence(mel, batch_first=True, padding_value=-20)\n",
    "    speaker = torch.stack(speaker)\n",
    "    return mel, speaker\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir_val,\n",
    "                   batch_size_val,\n",
    "                   n_workers_val,\n",
    "                   segment_len_val=128):\n",
    "    dataset = MyDataset(data_dir_val, segment_len=segment_len_val)\n",
    "    speaker_num_val = dataset.get_speaker_number()\n",
    "\n",
    "    train_length = int(0.9 * len(dataset))\n",
    "    lengths = [train_length, len(dataset) - train_length]\n",
    "    train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=n_workers_val,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=n_workers_val,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    return train_loader, valid_loader, speaker_num_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f79b4",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4c3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout_rate=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * expansion_factor)\n",
    "        self.swish = nn.SiLU()  # Swish activation function\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(d_model * expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_norm = self.layer_norm(x)\n",
    "\n",
    "        out = self.linear1(x_norm)\n",
    "        out = self.swish(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout2(out)\n",
    "        # The residual connection is handled in the ConformerBlock\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, kernel_size=15, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd for 'same' padding\"\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pointwise_conv1 = nn.Conv1d(\n",
    "            d_model,\n",
    "            2 * d_model,  # Expand to 2*d_model for GLU\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.glu = nn.GLU(\n",
    "            dim=1\n",
    "        )  # Gated Linear Unit, operates on the channel dimension (dim=1 for Conv1d)\n",
    "        self.depthwise_conv = nn.Conv1d(\n",
    "            d_model,  # Input channels for depthwise is d_model (after GLU)\n",
    "            d_model,  # Output channels is also d_model\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size - 1) // 2,  # 'same' padding\n",
    "            groups=d_model)  # Depthwise convolution\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model,\n",
    "                                         d_model,\n",
    "                                         kernel_size=1,\n",
    "                                         stride=1,\n",
    "                                         padding=0)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_norm = self.layer_norm(x)  # Pre-LayerNorm\n",
    "        x_conv = x_norm.transpose(\n",
    "            1, 2)  # (batch_size, d_model, sequence_length) for Conv1d\n",
    "\n",
    "        x_conv = self.pointwise_conv1(\n",
    "            x_conv)  # (batch_size, 2 * d_model, sequence_length)\n",
    "        x_conv = self.glu(x_conv)  # (batch_size, d_model, sequence_length)\n",
    "        x_conv = self.depthwise_conv(x_conv)\n",
    "        x_conv = self.batch_norm(\n",
    "            x_conv)  # BatchNorm after convolution, before activation\n",
    "        x_conv = self.swish(x_conv)\n",
    "        x_conv = self.pointwise_conv2(x_conv)\n",
    "        x_conv = self.dropout(x_conv)\n",
    "\n",
    "        x_conv = x_conv.transpose(1, 2)\n",
    "        # (batch_size, sequence_length, d_model)\n",
    "        # The residual connection is handled in the ConformerBlock\n",
    "        return x_conv\n",
    "\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            n_head,  # Renamed from num_heads to match MultiheadAttention's nhead\n",
    "            dim_feedforward_expansion=4,\n",
    "            conv_kernel_size=15,\n",
    "            dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        # First FeedForward Module\n",
    "        self.ffn1 = FeedForwardModule(d_model, dim_feedforward_expansion,\n",
    "                                      dropout_rate)\n",
    "\n",
    "        # Multi-Headed Self-Attention Module\n",
    "        self.attn_layer_norm = nn.LayerNorm(d_model)  # LayerNorm before MHSA\n",
    "        self.self_attn = nn.MultiheadAttention(d_model,\n",
    "                                               n_head,\n",
    "                                               dropout=dropout_rate,\n",
    "                                               batch_first=True)\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)  # Dropout after MHSA\n",
    "\n",
    "        # Convolution Module\n",
    "        self.conv_module = ConvolutionModule(d_model, conv_kernel_size,\n",
    "                                             dropout_rate)\n",
    "        # Second FeedForward Module\n",
    "        self.ffn2 = FeedForwardModule(d_model, dim_feedforward_expansion,\n",
    "                                      dropout_rate)\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(\n",
    "            d_model)  # Final LayerNorm for the block output\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None, src_mask=None):\n",
    "\n",
    "        # 1. First FeedForward Module (FFN1)\n",
    "        residual = x\n",
    "        ffn1_output = self.ffn1(x)  # FFN has LayerNorm internally\n",
    "        x = residual + 0.5 * ffn1_output  # As per Conformer paper, FFNs have 0.5 factor for residual\n",
    "\n",
    "        # 2. Multi-Headed Self-Attention Module (MHSA)\n",
    "        residual = x\n",
    "        x_norm_for_attn = self.attn_layer_norm(x)  # LayerNorm before MHSA\n",
    "        attn_output, _ = self.self_attn(query=x_norm_for_attn,\n",
    "                                        key=x_norm_for_attn,\n",
    "                                        value=x_norm_for_attn,\n",
    "                                        key_padding_mask=src_key_padding_mask,\n",
    "                                        attn_mask=src_mask)\n",
    "        x = residual + self.attn_dropout(attn_output)\n",
    "\n",
    "        # 3. Convolution Module (Conv)\n",
    "        residual = x\n",
    "        conv_output = self.conv_module(\n",
    "            x)  # Conv module has LayerNorm internally\n",
    "        x = residual + conv_output\n",
    "\n",
    "        # 4. Second FeedForward Module (FFN2)\n",
    "        residual = x\n",
    "        ffn2_output = self.ffn2(x)  # FFN has LayerNorm internally\n",
    "        x = residual + 0.5 * ffn2_output\n",
    "\n",
    "        x = self.final_layer_norm(x)  # Final LayerNorm for the block\n",
    "        return x\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(\n",
    "            0\n",
    "        )  # Shape: (1, max_len, d_model) for batch_first=True compatibility\n",
    "        self.register_buffer('pe',\n",
    "                             pe)  # Not a model parameter, but part of the state\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.pe shape: (1, max_len, d_model)\n",
    "        # Add positional encoding to x. Slice pe to match x's seq_len.\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,  # Internal dimension of the Conformer\n",
    "            num_encoder_layers,  # Number of ConformerBlocks\n",
    "            num_heads,  # Number of attention heads in ConformerBlock\n",
    "            dim_feedforward_expansion=4,  # Expansion factor for FFN in ConformerBlock\n",
    "            conv_kernel_size=15,  # Kernel size for ConvolutionModule in ConformerBlock\n",
    "            dropout_rate=0.0,\n",
    "            max_seq_len=5000,  # Maximum sequence length for positional encoding\n",
    "            input_dim=40,\n",
    "            num_speakers=600):\n",
    "        super().__init__()\n",
    "        # Project the dimension of features from input_dim into d_model.\n",
    "        self.prenet = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_seq_len,\n",
    "                                                        dropout_rate)\n",
    "\n",
    "        self.conformer_blocks = nn.ModuleList([\n",
    "            ConformerBlock(\n",
    "                d_model=d_model,\n",
    "                n_head=num_heads,  # Pass num_heads as n_head to ConformerBlock\n",
    "                dim_feedforward_expansion=dim_feedforward_expansion,\n",
    "                conv_kernel_size=conv_kernel_size,\n",
    "                dropout_rate=dropout_rate) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Project the dimension of features from d_model into num_speakers.\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),  # Using SiLU here as well for consistency, or ReLU\n",
    "            nn.Dropout(\n",
    "                dropout_rate),  # Dropout before the final classification layer\n",
    "            nn.Linear(d_model, num_speakers),\n",
    "        )\n",
    "\n",
    "    def forward(self, mels, src_key_padding_mask=None):\n",
    "\n",
    "        x = self.prenet(mels)  # (batch size, length, d_model)\n",
    "        x = self.pos_encoder(x)  # Add positional encoding\n",
    "\n",
    "        # Pass through Conformer blocks\n",
    "        # Note: src_mask for nn.MultiheadAttention in encoder is typically None unless you have specific masking needs\n",
    "        # other than padding. If src_key_padding_mask is provided, it will be used by self_attn.\n",
    "        for block in self.conformer_blocks:\n",
    "            x = block(x,\n",
    "                      src_key_padding_mask=src_key_padding_mask,\n",
    "                      src_mask=None)\n",
    "\n",
    "        # Mean pooling, considering padding if mask is provided\n",
    "        if src_key_padding_mask is not None:\n",
    "            # Invert mask: True for non-padded, False for padded\n",
    "            active_elements_mask = (~src_key_padding_mask\n",
    "                                   ).unsqueeze(-1).float()  # (batch, length, 1)\n",
    "            # Sum only non-padded elements and divide by the count of non-padded elements\n",
    "            masked_sum = (x * active_elements_mask).sum(dim=1)\n",
    "            num_active_elements = active_elements_mask.sum(dim=1).clamp(\n",
    "                min=1e-9)  # Avoid division by zero\n",
    "            stats = masked_sum / num_active_elements\n",
    "        else:\n",
    "            stats = x.mean(dim=1)  # (batch size, d_model)\n",
    "\n",
    "        out = self.pred_layer(stats)  # (batch size, num_speakers)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b617fc",
   "metadata": {},
   "source": [
    "## Fixing seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22104dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a7418",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8e1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# general paths and settings\n",
    "data_dir = \"./Dataset\"\n",
    "model_save_path = \"./speaker_model.ckpt\"\n",
    "submission_path = \"./submission.csv\"\n",
    "tensorboard_log_dir = \"./runs/\"\n",
    "seed = 3407\n",
    "\n",
    "# training parameters\n",
    "batch_size = 512\n",
    "num_workers = 0\n",
    "num_speakers = 600  # will be determined from the dataset\n",
    "\n",
    "# model parameters (transformer related)\n",
    "d_model = 256\n",
    "num_speakers = 600\n",
    "num_encoder_layers = 8\n",
    "dim_feedforward = d_model * 4\n",
    "num_heads = 4\n",
    "dropout_rate = 0.0\n",
    "\n",
    "# optimizer and scheduler parameters\n",
    "learning_rate = 1e-3\n",
    "num_warmup_steps = 5000\n",
    "total_steps = 100000\n",
    "\n",
    "# training control\n",
    "valid_steps = 2000\n",
    "early_stop_patience = 10\n",
    "\n",
    "# apply seed\n",
    "same_seeds(seed)\n",
    "\n",
    "# device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e378978",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49584337",
   "metadata": {},
   "source": [
    "### Warmup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a72321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    num_cycles: float = 0.5,\n",
    "    last_epoch: int = -1,\n",
    "):\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, 0.5 *\n",
    "            (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ed35b",
   "metadata": {},
   "source": [
    "### Validation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47c5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_speaker_model(valid_loader_val,\n",
    "                           model_val,\n",
    "                           criterion_val,\n",
    "                           device_val,\n",
    "                           current_step_or_epoch,\n",
    "                           writer_tb=None,\n",
    "                           pbar_desc_prefix=\"\"):\n",
    "    model_val.eval()\n",
    "    epoch_valid_loss = 0.0\n",
    "    epoch_valid_corrects = 0\n",
    "    num_valid_samples = 0\n",
    "\n",
    "    batch_pbar = tqdm(valid_loader_val,\n",
    "                      leave=False,\n",
    "                      desc=f\"{pbar_desc_prefix} Validation\")\n",
    "    with torch.no_grad():\n",
    "        for features, labels in batch_pbar:\n",
    "            features = features.to(device_val)\n",
    "            labels = labels.to(device_val)\n",
    "\n",
    "            outputs = model_val(features)\n",
    "            loss = criterion_val(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            epoch_valid_loss += loss.item() * features.size(0)\n",
    "            epoch_valid_corrects += (preds == labels).sum().item()\n",
    "            num_valid_samples += features.size(0)\n",
    "\n",
    "            batch_pbar.set_postfix(\n",
    "                loss=f\"{loss.item():.4f}\",\n",
    "                acc=f\"{(preds == labels).sum().item()/features.size(0):.2%}\")\n",
    "\n",
    "    avg_epoch_valid_loss = epoch_valid_loss / num_valid_samples if num_valid_samples > 0 else 0\n",
    "    avg_epoch_valid_acc = epoch_valid_corrects / num_valid_samples if num_valid_samples > 0 else 0\n",
    "\n",
    "    if writer_tb:\n",
    "        writer_tb.add_scalar('Loss/valid_step', avg_epoch_valid_loss,\n",
    "                             current_step_or_epoch)\n",
    "        writer_tb.add_scalar('Accuracy/valid_step', avg_epoch_valid_acc,\n",
    "                             current_step_or_epoch)\n",
    "\n",
    "    model_val.train()\n",
    "    return avg_epoch_valid_loss, avg_epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac59399",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93980106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_speaker(batch, model, criterion_val, device_val):\n",
    "    mels, labels = batch\n",
    "    mels = mels.to(device_val)\n",
    "    labels = labels.to(device_val)\n",
    "\n",
    "    outs = model(mels)\n",
    "    loss = criterion_val(outs, labels)\n",
    "\n",
    "    preds = outs.argmax(1)\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_speaker_model(model_to_train, train_loader_val, valid_loader_val,\n",
    "                        criterion_val, optimizer_val, scheduler_val,\n",
    "                        device_val):\n",
    "    tb_writer = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    best_model_state_dict = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_iterator = iter(train_loader_val)\n",
    "\n",
    "    epoch_running_loss = 0.0\n",
    "    epoch_running_corrects = 0\n",
    "    epoch_num_samples = 0\n",
    "    completed_data_passes = 0\n",
    "\n",
    "    pbar_total_steps = tqdm(range(total_steps), desc=\"Steps\")\n",
    "\n",
    "    for step in pbar_total_steps:\n",
    "        model_to_train.train()\n",
    "\n",
    "        try:\n",
    "            batch = next(train_iterator)\n",
    "        except StopIteration:\n",
    "            if epoch_num_samples > 0:\n",
    "                current_pass_avg_loss = epoch_running_loss / epoch_num_samples\n",
    "                current_pass_avg_acc = epoch_running_corrects / epoch_num_samples\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_scalar('Loss/train_data_pass_avg',\n",
    "                                         current_pass_avg_loss,\n",
    "                                         completed_data_passes + 1)\n",
    "                    tb_writer.add_scalar('Accuracy/train_data_pass_avg',\n",
    "                                         current_pass_avg_acc,\n",
    "                                         completed_data_passes + 1)\n",
    "\n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_corrects = 0\n",
    "            epoch_num_samples = 0\n",
    "            completed_data_passes += 1\n",
    "            train_iterator = iter(train_loader_val)\n",
    "            batch = next(train_iterator)\n",
    "\n",
    "        loss, accuracy = model_fn_speaker(batch, model_to_train, criterion_val,\n",
    "                                          device_val)\n",
    "\n",
    "        optimizer_val.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_val.step()\n",
    "        scheduler_val.step()\n",
    "\n",
    "        current_lr = optimizer_val.param_groups[0]['lr']\n",
    "\n",
    "        current_batch_size = batch[0].size(0)\n",
    "        epoch_running_loss += loss.item() * current_batch_size\n",
    "        epoch_running_corrects += accuracy.item() * current_batch_size\n",
    "        epoch_num_samples += current_batch_size\n",
    "\n",
    "        current_epoch_avg_train_acc = 0\n",
    "        if epoch_num_samples > 0:\n",
    "            current_epoch_avg_train_acc = epoch_running_corrects / epoch_num_samples\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            tb_writer.add_scalar('Loss/train_batch', loss.item(), step)\n",
    "            tb_writer.add_scalar('Accuracy/train_batch', accuracy.item(), step)\n",
    "            tb_writer.add_scalar('LearningRate/step', current_lr, step)\n",
    "\n",
    "        pbar_total_steps.set_postfix(\n",
    "            batch_loss=f\"{loss.item():.4f}\",\n",
    "            best_acc=f\"{best_valid_acc:.2%}\",\n",
    "            run_avg_acc=f\"{current_epoch_avg_train_acc:.2%}\",\n",
    "            lr=f\"{current_lr:.2e}\")\n",
    "\n",
    "        if (step + 1) % valid_steps == 0:\n",
    "            avg_valid_loss, avg_valid_acc = validate_speaker_model(\n",
    "                valid_loader_val,\n",
    "                model_to_train,\n",
    "                criterion_val,\n",
    "                device_val,\n",
    "                step + 1,\n",
    "                tb_writer,\n",
    "                pbar_desc_prefix=f\"Step {step+1}\")\n",
    "\n",
    "            pbar_total_steps.write(\n",
    "                f\"Step {step + 1}/{total_steps} - \"\n",
    "                f\"Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {avg_valid_acc:.2%}\"\n",
    "            )\n",
    "\n",
    "            if avg_valid_acc > best_valid_acc:\n",
    "                best_valid_acc = avg_valid_acc\n",
    "                best_model_state_dict = model_to_train.state_dict()\n",
    "                torch.save(best_model_state_dict, model_save_path)\n",
    "                pbar_total_steps.write(\n",
    "                    f\"Best model updated at step {step + 1}. Accuracy: {best_valid_acc:.2%}. Saved to {model_save_path}\"\n",
    "                )\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                pbar_total_steps.write(\n",
    "                    f\"Early stopping triggered at step {step + 1} after {epochs_no_improve} validation checks without improvement.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    tb_writer.close()\n",
    "    print(\n",
    "        f\"\\nTraining finished. Best validation accuracy: {best_valid_acc:.4%}\")\n",
    "    if best_valid_acc > 0:\n",
    "        print(f\"Best model saved to {model_save_path}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"No model was saved as validation accuracy did not improve or training was too short.\"\n",
    "        )\n",
    "\n",
    "    return best_valid_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9387a",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9865557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # prepare dataloaders\n",
    "    train_loader, valid_loader, speaker_num_from_data = get_dataloader(\n",
    "        data_dir_val=data_dir,\n",
    "        batch_size_val=batch_size,\n",
    "        n_workers_val=num_workers)\n",
    "\n",
    "    print(\n",
    "        f\"Train Dataloader: {len(train_loader.dataset)} samples, {len(train_loader)} batches\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Valid Dataloader: {len(valid_loader.dataset)} samples, {len(valid_loader)} batches\"\n",
    "    )\n",
    "\n",
    "    # initialize model, criterion, optimizer, and scheduler\n",
    "    print(\"Initializing model, criterion, optimizer, and scheduler...\")\n",
    "    speaker_model = Classifier(d_model=d_model,\n",
    "                               num_encoder_layers=num_encoder_layers,\n",
    "                               num_heads=num_heads,\n",
    "                               dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(speaker_model.parameters(), lr=learning_rate)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps,\n",
    "                                                total_steps)\n",
    "\n",
    "    # Run training\n",
    "    best_accuracy_from_training = train_speaker_model(\n",
    "        model_to_train=speaker_model,\n",
    "        train_loader_val=train_loader,\n",
    "        valid_loader_val=valid_loader,\n",
    "        criterion_val=criterion,\n",
    "        optimizer_val=optimizer,\n",
    "        scheduler_val=scheduler,\n",
    "        device_val=device)\n",
    "    print(\n",
    "        f\"Training complete. Best validation accuracy: {best_accuracy_from_training:.2%}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2de4c0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f95fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataloader: 50999 samples, 99 batches\n",
      "Valid Dataloader: 5667 samples, 12 batches\n",
      "Initializing model, criterion, optimizer, and scheduler...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62650a69dfa3408babe38c35156bce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b6872fe4674574b34a2f3011bec6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 2000 Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/100000 - Valid Loss: 1.3955, Valid Acc: 67.09%\n",
      "Best model updated at step 2000. Accuracy: 67.09%. Saved to ./speaker_model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03446697aef84898a2d9bbb370aabfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 4000 Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/100000 - Valid Loss: 1.3886, Valid Acc: 69.65%\n",
      "Best model updated at step 4000. Accuracy: 69.65%. Saved to ./speaker_model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114682b0b7e54cce8177c95c4cfc3c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 6000 Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000/100000 - Valid Loss: 0.9719, Valid Acc: 78.31%\n",
      "Best model updated at step 6000. Accuracy: 78.31%. Saved to ./speaker_model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01064580a5224f9e97f44ab2a3c56bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 8000 Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000/100000 - Valid Loss: 0.7737, Valid Acc: 83.15%\n",
      "Best model updated at step 8000. Accuracy: 83.15%. Saved to ./speaker_model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32388258c33848feb7e100de5757d3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Step 10000 Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000/100000 - Valid Loss: 0.6571, Valid Acc: 85.87%\n",
      "Best model updated at step 10000. Accuracy: 85.87%. Saved to ./speaker_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25faedd3",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/  --port 6008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f287e7",
   "metadata": {},
   "source": [
    "# Test & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa76ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Finish loading data!\n",
      "[Info]: Finish creating model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42749/3801919637.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad94288d7048f2bfcf1893cf9befca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inference Dataset\n",
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir_val):\n",
    "        testdata_path = Path(data_dir_val) / \"testdata.json\"\n",
    "        metadata = json.load(testdata_path.open())\n",
    "        self.data_dir = data_dir_val\n",
    "        self.data = metadata[\"utterances\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        utterance = self.data[index]\n",
    "        feat_path = utterance[\"feature_path\"]\n",
    "        mel = torch.load(os.path.join(self.data_dir, feat_path), weights_only=True)\n",
    "        mel = torch.FloatTensor(mel)\n",
    "        return feat_path, mel\n",
    "\n",
    "\n",
    "# inference collate function\n",
    "def inference_collate_batch(batch):\n",
    "    feat_paths, mels = zip(*batch)\n",
    "    mels_padded = pad_sequence(mels, batch_first=True, padding_value=-20)\n",
    "    return feat_paths, mels_padded\n",
    "\n",
    "\n",
    "# create submission file\n",
    "def create_submission(device_val):\n",
    "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "    mapping = json.load(mapping_path.open())\n",
    "\n",
    "    dataset = InferenceDataset(data_dir)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=inference_collate_batch,\n",
    "    )\n",
    "    print(f\"[Info]: Finish loading data!\", flush=True)\n",
    "\n",
    "    speaker_num = len(mapping[\"id2speaker\"])\n",
    "    model = Classifier(d_model=d_model,\n",
    "                       num_encoder_layers=num_encoder_layers,\n",
    "                       num_heads=num_heads,\n",
    "                       dropout_rate=0).to(device)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "    print(f\"[Info]: Finish creating model!\", flush=True)\n",
    "\n",
    "    results = [[\"Id\", \"Category\"]]\n",
    "    for feat_paths, mels in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            mels = mels.to(device)\n",
    "            outs = model(mels)\n",
    "            preds = outs.argmax(1).cpu().numpy()\n",
    "            for feat_path, pred in zip(feat_paths, preds):\n",
    "                results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
    "\n",
    "    with open(submission_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(results)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "create_submission(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f56f98-fb13-4c3b-ace2-3bfd10ef2130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
