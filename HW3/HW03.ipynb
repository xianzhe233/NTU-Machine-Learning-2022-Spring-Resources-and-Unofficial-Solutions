{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e626e2f",
   "metadata": {},
   "source": [
    "# HW3 Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407095f3",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be978e",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L \"https://www.dropbox.com/s/6l2vcvxl54b0b6w/food11.zip\" -o food11.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271271a4",
   "metadata": {},
   "source": [
    "### Manually Unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fcff53",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c1e40",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67c758",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de044a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_tfm = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_tfm_without_norm = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([train_tfm_without_norm, normalize_tfm])\n",
    "\n",
    "test_tfm_without_norm = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([test_tfm_without_norm, normalize_tfm])\n",
    "\n",
    "tta_tfm = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(30)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1c4d2",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d72b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, tfm=test_tfm, files=None):\n",
    "        super(FoodDataset).__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([\n",
    "            os.path.join(path, x)\n",
    "            for x in os.listdir(path)\n",
    "            if x.endswith(\".jpg\")\n",
    "        ])\n",
    "        if files:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\", self.files[0])\n",
    "        self.transform = tfm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.files[index]\n",
    "        image = Image.open(file_name)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        try:\n",
    "            # name of an image:  .../[label]_[id].jpg\n",
    "            base_name = os.path.basename(file_name)\n",
    "            label = int(base_name.split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b3b0",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # input dim: [3, 128, 128]\n",
    "        model = models.efficientnet_b0(\n",
    "            weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        num_features = model.classifier[-1].out_features\n",
    "        model.classifier.append(nn.Linear(num_features, 11))\n",
    "        self.cnn = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750802e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "seed = 3407\n",
    "\n",
    "num_epoch = 500\n",
    "early_stop = 20\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "dropout_rate = 0.0\n",
    "\n",
    "tta_iterations = 10\n",
    "origin_weight = 5\n",
    "\n",
    "dataset_dir = './food11'\n",
    "model_path = './model.ckpt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74fa62",
   "metadata": {},
   "source": [
    "### Fixing seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb92f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "same_seeds(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1497a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512aaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2b2c4",
   "metadata": {},
   "source": [
    "### Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=learning_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c57fad",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FoodDataset(os.path.join(dataset_dir, \"training\"), tfm=train_tfm)\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True)\n",
    "valid_set = FoodDataset(os.path.join(dataset_dir, \"validation\"), tfm=test_tfm)\n",
    "valid_loader = DataLoader(valid_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0267b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_epoch(model, train_loader, criterion, optimizer, device,\n",
    "                    current_epoch_num, global_step_ref):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_corrects = 0\n",
    "    num_train_samples = 0\n",
    "\n",
    "    batch_pbar = tqdm(train_loader,\n",
    "                      leave=False,\n",
    "                      desc=f\"Epoch {current_epoch_num} Training\")\n",
    "    for features, labels in batch_pbar:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        global_step_ref[0] += 1\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        epoch_train_loss += loss.item() * features.size(0)\n",
    "        epoch_train_corrects += (preds.detach() == labels.detach()).sum().item()\n",
    "        num_train_samples += features.size(0)\n",
    "\n",
    "    avg_epoch_train_loss = epoch_train_loss / num_train_samples if num_train_samples > 0 else 0\n",
    "    avg_epoch_train_acc = epoch_train_corrects / num_train_samples if num_train_samples > 0 else 0\n",
    "\n",
    "    return avg_epoch_train_loss, avg_epoch_train_acc\n",
    "\n",
    "\n",
    "def run_validation_epoch(model, valid_loader, criterion, device,\n",
    "                         current_epoch_num):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0.0\n",
    "    epoch_valid_corrects = 0\n",
    "    num_valid_samples = 0\n",
    "\n",
    "    batch_pbar = tqdm(valid_loader,\n",
    "                      leave=False,\n",
    "                      desc=f\"Epoch {current_epoch_num} Validation\")\n",
    "    with torch.no_grad():\n",
    "        for features, labels in batch_pbar:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            epoch_valid_loss += loss.item() * features.size(0)\n",
    "            epoch_valid_corrects += (preds.cpu() == labels.cpu()).sum().item()\n",
    "            num_valid_samples += features.size(0)\n",
    "\n",
    "    avg_epoch_valid_loss = epoch_valid_loss / num_valid_samples if num_valid_samples > 0 else 0\n",
    "    avg_epoch_valid_acc = epoch_valid_corrects / num_valid_samples if num_valid_samples > 0 else 0\n",
    "\n",
    "    return avg_epoch_valid_loss, avg_epoch_valid_acc\n",
    "\n",
    "\n",
    "def update_epoch_summary(pbar_epoch, writer_tb, current_epoch, train_loss,\n",
    "                         train_acc, valid_loss, valid_acc, current_lr,\n",
    "                         best_acc_so_far, not_improving_count):\n",
    "    writer_tb.add_scalar('Loss/train_epoch', train_loss, current_epoch)\n",
    "    writer_tb.add_scalar('Accuracy/train_epoch', train_acc, current_epoch)\n",
    "    writer_tb.add_scalar('Loss/valid_epoch', valid_loss, current_epoch)\n",
    "    writer_tb.add_scalar('Accuracy/valid_epoch', valid_acc, current_epoch)\n",
    "    writer_tb.add_scalar('LearningRate/epoch', current_lr, current_epoch)\n",
    "\n",
    "    info_str = (\n",
    "        f\"BestAcc: {best_acc_so_far:.2%}, TrainLoss: {train_loss:.4f}, TrainAcc: {train_acc:.2%}, \"\n",
    "        f\"ValidLoss: {valid_loss:.4f}, ValidAcc: {valid_acc:.2%}, LR: {current_lr:.2e}\"\n",
    "    )\n",
    "    if not_improving_count > 0:\n",
    "        info_str += f\", NoImprove: {not_improving_count}ep\"\n",
    "    pbar_epoch.set_postfix_str(info_str)\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                num_epoch,\n",
    "                device,\n",
    "                model_path,\n",
    "                early_stop,\n",
    "                tensorboard_log_dir='./runs'):\n",
    "    best_val_acc = 0.0\n",
    "    not_improving_epochs = 0\n",
    "    global_step_counter = [0]\n",
    "\n",
    "    epoch_pbar = trange(num_epoch, desc=\"Total Epochs\")\n",
    "    tb_writer = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        avg_train_loss, avg_train_acc = run_train_epoch(model, train_loader,\n",
    "                                                        criterion, optimizer,\n",
    "                                                        device, epoch + 1,\n",
    "                                                        global_step_counter)\n",
    "\n",
    "        avg_valid_loss, avg_valid_acc = run_validation_epoch(\n",
    "            model, valid_loader, criterion, device, epoch + 1)\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_valid_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if avg_valid_acc > best_val_acc:\n",
    "            not_improving_epochs = 0\n",
    "            best_val_acc = avg_valid_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            not_improving_epochs += 1\n",
    "\n",
    "        update_epoch_summary(epoch_pbar, tb_writer, epoch + 1, avg_train_loss,\n",
    "                             avg_train_acc, avg_valid_loss, avg_valid_acc,\n",
    "                             current_learning_rate, best_val_acc,\n",
    "                             not_improving_epochs)\n",
    "\n",
    "        if not_improving_epochs >= early_stop:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    tb_writer.close()\n",
    "    print(f\"\\nTraining finished. Best validation accuracy: {best_val_acc:.4%}\")\n",
    "    if best_val_acc > 0 or not_improving_epochs < num_epoch:\n",
    "        print(f\"Best model saved to {model_path}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"No model was saved as validation accuracy did not improve over initial. Check {model_path} for pre-existing files.\"\n",
    "        )\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c5ebe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4288083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler,\n",
    "            num_epoch, device, model_path, early_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d42bed",
   "metadata": {},
   "source": [
    "# Test & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/  --port 6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa979c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = FoodDataset(os.path.join(dataset_dir, \"test\"), tfm=test_tfm)\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=0,\n",
    "                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Classifier().to(device)\n",
    "best_model.load_state_dict(torch.load(\"model.ckpt\"))\n",
    "best_model.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for image, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        origin_image = image.to(device)\n",
    "        origin_pred = best_model(origin_image)\n",
    "        origin_label = torch.argmax(origin_pred, dim=1)\n",
    "        voting_predictions = [origin_label] * origin_weight\n",
    "\n",
    "        for _ in range(tta_iterations):\n",
    "            transformed_image = tta_tfm(image)\n",
    "            transformed_image = transformed_image.to(device)\n",
    "\n",
    "            tta_pred = best_model(transformed_image)\n",
    "            tta_label = torch.argmax(tta_pred, dim=1)\n",
    "            voting_predictions.append(tta_label)\n",
    "\n",
    "        stacked_predictions = torch.stack(voting_predictions, dim=1)\n",
    "        vote_res = torch.mode(stacked_predictions, dim=1).values\n",
    "        vote_label = vote_res.cpu().data.numpy()\n",
    "\n",
    "        prediction += vote_label.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79779402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test csv\n",
    "def pad4(i):\n",
    "    return \"0\" * (4 - len(str(i))) + str(i)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Id\"] = [pad4(i) for i in range(1, len(test_set) + 1)]\n",
    "df[\"Category\"] = prediction\n",
    "df.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
